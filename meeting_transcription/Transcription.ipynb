{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c295885e-8f86-4996-bdab-a4f778f6dbe1",
   "metadata": {},
   "source": [
    "## Inputs and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298f3dc-2b10-4b2b-9205-eb7a49806178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input params\n",
    "# Provide either local file path or YouTube URL\n",
    "INPUT_VIDEO_FILE=\"\" \n",
    "INPUT_YOUTUBE_URL=\"\" # \"https://www.youtube.com/watch?v=4V2C0X4qqLY\"\n",
    "\n",
    "# # Tokens, etc\n",
    "# Hugging Face token: https://huggingface.co/docs/hub/security-tokens#user-access-tokens \n",
    "HUGGINGFACE_AUTH_TOKEN=\"your Hugging Face Auth Token...\"\n",
    "\n",
    "# # Model params\n",
    "\n",
    "# Model size: tiny, base, small, medium, large, large-v2\n",
    "# Can also have \".en\" suffix for English-specific model; e.g. base.en\n",
    "WHISPER_MODEL = \"base.en\"\n",
    "\n",
    "# Merge tracks with same label and separated by less than COLLAR seconds.\n",
    "COLLAR_IN_SECS = 0.5\n",
    "\n",
    "# Output files\n",
    "OUTPUT_TRANSCRIPTION=\"output.sub\"\n",
    "\n",
    "# Temporary files used in the process\n",
    "TEMP_VIDEO_FILE=\"temp/input.mp4\"\n",
    "TEMP_AUDIO_FILE=\"temp/input.wav\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c47e7cf-d634-4786-9e37-ac7b81506c99",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958404c2-2948-4a55-878b-98f641f6a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, pipeline\n",
    "from pyannote.audio import Pipeline\n",
    "from pytube import YouTube\n",
    "from pydub import AudioSegment\n",
    "import datetime\n",
    "import moviepy.editor as mp\n",
    "import os, shutil\n",
    "\n",
    "def ensure_dir(path):\n",
    "    \"\"\"Make sure director from the given path exists\"\"\"\n",
    "    \n",
    "    dir = os.path.dirname(path)\n",
    "    if dir:\n",
    "        os.makedirs(dir, exist_ok=True)\n",
    "\n",
    "def fetch_youtube_audio(url):\n",
    "    \"\"\"Fetch WAV audio from given youtube URL\"\"\"\n",
    "    \n",
    "    print(\"Fetching audio from Youtube URL:\", url)\n",
    "\n",
    "    ensure_dir(TEMP_VIDEO_FILE)\n",
    "    ensure_dir(TEMP_AUDIO_FILE)\n",
    "\n",
    "    video_stream = YouTube(url).streams.first()\n",
    "    video_stream.download(filename=TEMP_VIDEO_FILE)\n",
    "\n",
    "    video = mp.VideoFileClip(TEMP_VIDEO_FILE)\n",
    "    video.audio.write_audiofile(TEMP_AUDIO_FILE, codec='pcm_s16le')\n",
    "\n",
    "    print(\"Done fetching audio form YouTube to file: \", TEMP_AUDIO_FILE)\n",
    "    return TEMP_AUDIO_FILE\n",
    "\n",
    "\n",
    "def extract_wav_from_video(video_file):\n",
    "    \"\"\"Extract WAV audio from given video file\"\"\"\n",
    "    \n",
    "    print(\"Extracting audio from video file\", video_file)\n",
    "\n",
    "    ensure_dir(TEMP_AUDIO_FILE)\n",
    "    video = mp.VideoFileClip(video_file)\n",
    "    video.audio.write_audiofile(TEMP_AUDIO_FILE, codec='pcm_s16le')\n",
    "\n",
    "    print(\"Done fetching audio form YouTube to file: \", TEMP_AUDIO_FILE)\n",
    "    return TEMP_AUDIO_FILE\n",
    "\n",
    "\n",
    "TIMESTAMP_FORMAT = \"%H:%M:%S.%f\"\n",
    "base_time = datetime.datetime(1970, 1, 1)\n",
    "\n",
    "def format_timestamp(seconds):\n",
    "    \"\"\"Format timestamp in SubViewer format: https://wiki.videolan.org/SubViewer/\"\"\"\n",
    "    \n",
    "    date = base_time + datetime.timedelta(seconds=seconds)\n",
    "    return date.strftime(TIMESTAMP_FORMAT)[:-4]\n",
    "\n",
    "def extract_audio_track(input_file, output_file, start_time, end_time):\n",
    "    \"\"\"Extract and save part of given audio file\"\"\"\n",
    "    \n",
    "    # Load the WAV file\n",
    "    audio = AudioSegment.from_wav(input_file)\n",
    "\n",
    "    # Calculate the start and end positions in milliseconds\n",
    "    start_ms = start_time * 1000\n",
    "    end_ms = end_time * 1000\n",
    "\n",
    "    # Extract the desired segment\n",
    "    track = audio[start_ms:end_ms]\n",
    "\n",
    "    track.export(output_file, format=\"mp3\")\n",
    "\n",
    "def generate_speaker_diarization(audio_file):\n",
    "    \"\"\"Generate speaker diarization for given audio file\"\"\"\n",
    "    \n",
    "    print(\"Generating speaker diarization... audio_file=\", audio_file)\n",
    "\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "      \"pyannote/speaker-diarization-3.0\",\n",
    "      use_auth_token=HUGGINGFACE_AUTH_TOKEN)\n",
    "    \n",
    "    result = pipeline(audio_file)\n",
    "    \n",
    "    print(\"Done generating spearer diarization\")\n",
    "    return result\n",
    "\n",
    "def generate_transcription(diarization):\n",
    "    \"\"\"Generate transcription from given diarization object\"\"\"\n",
    "    \n",
    "    print(\"Generating transcription model: \", WHISPER_MODEL)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=f\"openai/whisper-{WHISPER_MODEL}\",\n",
    "        chunk_length_s=30,\n",
    "        device=\"mps\"\n",
    "    )\n",
    "\n",
    "    # Create directory for tracks\n",
    "    shutil.rmtree(\"output-tracks\", ignore_errors=True)\n",
    "    os.mkdir(\"output-tracks\")\n",
    "\n",
    "    result = []\n",
    "    for turn, _, speaker in diarization.support(COLLAR_IN_SECS).itertracks(yield_label=True):\n",
    "        track_file = f\"output-tracks/{round(turn.start, 2)}-{speaker}.mp3\"\n",
    "        track_path = os.path.join(os.curdir, track_file)\n",
    "        extract_audio_track(TEMP_AUDIO_FILE, track_file, turn.start, turn.end, )\n",
    "        \n",
    "        track_data = None\n",
    "        with open(track_file, \"rb\") as audio_content:\n",
    "            track_data = audio_content.read()\n",
    "\n",
    "        output = pipe(track_data, batch_size=8, return_timestamps=False)\n",
    "        text = output['text']\n",
    "            \n",
    "        result.append({\n",
    "            'start': turn.start,\n",
    "            'end': turn.end,\n",
    "            'speaker': speaker, \n",
    "            'text': text.strip(), \n",
    "            'track_path': track_path\n",
    "        })\n",
    "\n",
    "    print(\"Done generating transcripion. Parts: \", len(result))\n",
    "    return result\n",
    "\n",
    "def format_transcription(transcription):\n",
    "    \"\"\"Format transcription in SubViewer format: https://wiki.videolan.org/SubViewer/\"\"\"\n",
    "    \n",
    "    result = \"\"\n",
    "    for t in transcription:\n",
    "        result += f\"{format_timestamp(t['start'])},{format_timestamp(t['end'])}\\n{t['speaker']}: {t['text']}\\n\\n\"\n",
    "    return result\n",
    "\n",
    "def save_transcription(transcription, output_file):\n",
    "    \"\"\"Save trainscription in SubViewer format to file.\"\"\"\n",
    "    \n",
    "    print(\"Saving transcripion to file...\", output_file)\n",
    "    \n",
    "    f = open(output_file, \"w\")\n",
    "    f.write(format_transcription(transcription))\n",
    "    f.close()\n",
    "\n",
    "    print(\"Done saving transcripion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce11041-30f6-4dd1-8fd0-002e88c2469f",
   "metadata": {},
   "source": [
    "## Main program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915f616-d4bb-4767-a221-7c83afe7f72a",
   "metadata": {},
   "source": [
    "### Fetch video / Extract Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3d389-907e-47c2-b24b-54df69556002",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if INPUT_VIDEO_FILE:\n",
    "    audio_file = extract_wav_from_video(INPUT_VIDEO_FILE)\n",
    "elif INPUT_YOUTUBE_URL:\n",
    "    audio_file = fetch_youtube_audio(INPUT_YOUTUBE_URL)\n",
    "else:\n",
    "    print(\"Set INPUT_VIDEO_FILE or INPUT_YOUTUBE_URL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1bb2c4-359e-4440-a231-899bf3d0cd71",
   "metadata": {},
   "source": [
    "### Generate diarizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4351eb-df31-411d-95b4-cd30cd572802",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "diarization = generate_speaker_diarization(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c30a317-4f8f-4dcf-9a0b-9883e4178355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c63855-80dd-4b18-b10b-004173053b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transcription = generate_transcription(diarization)\n",
    "save_transcription(transcription, OUTPUT_TRANSCRIPTION)\n",
    "\n",
    "print(format_transcription(transcription))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49882884-3d6e-4a08-bc0c-083e75d546b7",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
